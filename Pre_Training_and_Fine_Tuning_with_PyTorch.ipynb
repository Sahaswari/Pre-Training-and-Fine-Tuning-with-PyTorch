{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeRw5jlWCpd7TTEgb4D4KG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahaswari/Pre-Training-and-Fine-Tuning-with-PyTorch/blob/main/Pre_Training_and_Fine_Tuning_with_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "install libraries"
      ],
      "metadata": {
        "id": "OjKPO-oWIF1t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvplXpr2FgV6",
        "outputId": "43ee3846-492f-4671-961b-eee9181e274c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Downloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Downloading torch-2.2.2-cp311-cp311-manylinux1_x86_64.whl (755.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\n",
            "Collecting torchtext==0.17.2\n",
            "  Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.32.3)\n",
            "Requirement already satisfied: torch==2.2.2 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.17.2) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.2.2->torchtext==0.17.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.17.2) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
            "Downloading torchtext-0.17.2-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.17.2\n",
            "Collecting portalocker==2.8.2\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl.metadata (8.5 kB)\n",
            "Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.8.2\n",
            "Collecting torchdata==0.7.1\n",
            "  Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.32.3)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.1) (2.2.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchdata==0.7.1) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata==0.7.1) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchdata==0.7.1) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchdata==0.7.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2->torchdata==0.7.1) (1.3.0)\n",
            "Downloading torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchdata\n",
            "Successfully installed torchdata-0.7.1\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting matplotlib==3.9.0\n",
            "  Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scikit-learn==1.5.0\n",
            "  Downloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.9.0) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.0) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib==3.9.0) (1.17.0)\n",
            "Downloading matplotlib-3.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn, matplotlib\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "Successfully installed matplotlib-3.9.0 scikit-learn-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.2.2\n",
        "!pip install torchtext==0.17.2\n",
        "!pip install portalocker==2.8.2\n",
        "!pip install torchdata==0.7.1\n",
        "!pip install pandas\n",
        "!pip install matplotlib==3.9.0 scikit-learn==1.5.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX5bR1WQIPpU",
        "outputId": "2b53bb62-705b-421a-ed43-9a15bdbfa134"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.0\n",
            "  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.2.2 which is incompatible.\n",
            "langchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Import required libraries"
      ],
      "metadata": {
        "id": "NSnw3dejIRq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from itertools import accumulate\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "import torch\n",
        "torch.set_num_threads(1)\n",
        "from torch import nn\n",
        "import os\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from IPython.display import Markdown as md\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, GloVe, Vectors\n",
        "from torchtext.datasets import IMDB\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "import pickle\n",
        "\n",
        "from urllib.request import urlopen\n",
        "import io\n",
        "\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "GyB0BnqyIUaa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Define helper functions"
      ],
      "metadata": {
        "id": "s2rBOKm-Ifrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(COST,ACC):\n",
        "\n",
        "    fig, ax1 = plt.subplots()\n",
        "    color = 'tab:red'\n",
        "    ax1.plot(COST, color=color)\n",
        "    ax1.set_xlabel('epoch', color=color)\n",
        "    ax1.set_ylabel('total loss', color=color)\n",
        "    ax1.tick_params(axis='y', color=color)\n",
        "\n",
        "    ax2 = ax1.twinx()\n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('accuracy', color=color)  # you already handled the x-label with ax1\n",
        "    ax2.plot(ACC, color=color)\n",
        "    ax2.tick_params(axis='y', color=color)\n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ClPnddExIh-z"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_list_to_file(lst, filename):\n",
        "    \"\"\"\n",
        "    Save a list to a file using pickle serialization.\n",
        "\n",
        "    Parameters:\n",
        "        lst (list): The list to be saved.\n",
        "        filename (str): The name of the file to save the list to.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(lst, file)\n",
        "\n",
        "def load_list_from_file(filename):\n",
        "    \"\"\"\n",
        "    Load a list from a file using pickle deserialization.\n",
        "\n",
        "    Parameters:\n",
        "        filename (str): The name of the file to load the list from.\n",
        "\n",
        "    Returns:\n",
        "        list: The loaded list.\n",
        "    \"\"\"\n",
        "    with open(filename, 'rb') as file:\n",
        "        loaded_list = pickle.load(file)\n",
        "    return loaded_list"
      ],
      "metadata": {
        "id": "l4YNf-5ZJAqp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Positional encodings"
      ],
      "metadata": {
        "id": "2XVjzj5_JEJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, vocab_size=5000, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(vocab_size, d_model)\n",
        "        position = torch.arange(0, vocab_size, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float()\n",
        "            * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "zmn7VRkCJFBb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Import IMDB dataset"
      ],
      "metadata": {
        "id": "xDUR6dRoKxYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/35t-FeC-2uN1ozOwPs7wFg.gz')\n",
        "tar = tarfile.open(fileobj=io.BytesIO(urlopened.read()))\n",
        "tempdir = tempfile.TemporaryDirectory()\n",
        "tar.extractall(tempdir.name)\n",
        "tar.close()"
      ],
      "metadata": {
        "id": "vVdwISTHKyFc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMDB dataset overview"
      ],
      "metadata": {
        "id": "m4oFwu4JK8xV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, root_dir, train=True):\n",
        "        \"\"\"\n",
        "        root_dir: The base directory of the IMDB dataset.\n",
        "        train: A boolean flag indicating whether to use training or test data.\n",
        "        \"\"\"\n",
        "        self.root_dir = os.path.join(root_dir, \"train\" if train else \"test\")\n",
        "        self.neg_files = [os.path.join(self.root_dir, \"neg\", f) for f in os.listdir(os.path.join(self.root_dir, \"neg\")) if f.endswith('.txt')]\n",
        "        self.pos_files = [os.path.join(self.root_dir, \"pos\", f) for f in os.listdir(os.path.join(self.root_dir, \"pos\")) if f.endswith('.txt')]\n",
        "        self.files = self.neg_files + self.pos_files\n",
        "        self.labels = [0] * len(self.neg_files) + [1] * len(self.pos_files)\n",
        "        self.pos_inx=len(self.pos_files)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_path = self.files[idx]\n",
        "        label = self.labels[idx]\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read()\n",
        "        return label, content"
      ],
      "metadata": {
        "id": "me9mEGR5K9mp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = tempdir.name + '/' + 'imdb_dataset'\n",
        "train_iter = IMDBDataset(root_dir=root_dir, train=True)  # For training data\n",
        "test_iter = IMDBDataset(root_dir=root_dir, train=False)  # For test data\n",
        "\n",
        "start=train_iter.pos_inx\n",
        "for i in range(-10,10):\n",
        "    print(train_iter[start+i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsA4CSjDLcNN",
        "outputId": "148d3966-2293-4c11-89e3-7761cc4dee2c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, \"Just saw this movie, and what a waste of time. The movie was predictable and slow. It's basically the Mormon bad news bears that play church sanctioned basketball. Rather than watching this movie, I should have had a root canal. The cameo performances were obviously driven by sponsorship / funding. This movie had potential due to the outrageous behavior that is exhibited by Mormons when they play church sanctioned basketball, however because it's rated PG, the true nature of the spectacle could not be transfered to film. The acting is horrible with the exception of Clint Howard and Fred Willard. Thurl Bailey's appearance in the film was completely unnecessary.\")\n",
            "(0, 'I knew my summary would get you. How is this movie like a Pet Rock and Disco?! Well, unless you lived through the 1970s or 80s, you probably can\\'t understand WHY anyone would like a New Coke or own a Pet Rock (and frankly, at least in the case of Pet Rocks, I STILL don\\'t understand it completely). They\\'re just a couple things that seemed to make sense at the time but really baffle the younger generation. The same can be said for Kay Kyser and his band. At the time (the 1940s mostly), they were very popular and had enough clout that the studio starred them with Boris Karloff, Bela Lugosi AND Peter Lorre in this film. Yet, if you didn\\'t live at that time (it was well before my time), you wonder why anyone liked this sort of \"entertainment\". After all, Kyser and his band mates are incredibly obnoxious and their humor is very, very broad (i.e., unsophisticated and cheesy). Frankly, I couldn\\'t stand their antics nor did I appreciate that there were just too many musical numbers in the film. Because of these factors, the great supporting cast was given a back seat and fans of these actors will probably be disappointed.<br /><br />The film involves Kyser and the band coming to a mansion where a young lady and her wacky aunt live. Once there, the bridge is washed out and strange happenings begin. Eventually, it culminates in some attempts on Sally\\'s life and a séance (of sorts). It\\'s all played for laughs--and it\\'s really not a horror movie despite the cast.<br /><br />Overall, it\\'s passable entertainment at best. As a Lugosi and Karloff fan, I sure felt cheated having to watch Kyser and his knuckleheads.')\n",
            "(0, \"Going into this I was expecting anything really good, but after the damage this inflexed on me, I'm just happy to think strait. It's hard to think what the film-makers( HA!) this was a good movie. the stories, and I use the world loosely, are incoherent and do make any sense at all. There just stupid things that happen at random. the acting, if can be called acting is horrible I've seen batter acting in toy ads! I know it's a low-budget video-bin garbages, but still even it's not like they tried. Will after stetting thought it, I feel very sleepy and still #yawws# do, I'm going to go lie down.<br /><br />WARNNING: DO NOT ATEMP TO DRIVE, WALK, READ OR DO ANY AFTER Watching CHILLERS. OTHER SIDE AFFECTTS MAY ENGULED LOSE OF ANY OR ALL METAL FUNKIONS.\")\n",
            "(0, \"This has to be some of the worst direction I've seen. The close-up can be a very powerful shot, but when every scene consists of nothing but close-ups, it loses all its impact. <br /><br />Tony Scott has some very beautiful scenery to work with, the backdrops of Mexico, the cantinas, the beautiful estate where Anthony Quinn lives, and the dusty towns Costner rolls through on his journey for revenge. Unfortunately we only catch quick glimpses of these places before the camera cuts to a picture of a big, giant head. Even the transition scenes where Costner is driving alone across Mexico quickly cut to a close-up. <br /><br />The score is over-dramatic and intrusive, dictating every emotion we should feel. The story itself should have been handled much better. Among other things, too many people pop up out of nowhere to help Costner along - it's just bad writing. <br /><br />It's a typical thriller storyline, but many others have taken the same premise and done outstanding things with it. Costner's No Way Out had a somewhat similar storyline, but it was a much better movie. <br /><br />The ending was completely anticlimactic and suffered from the most melodramatic scoring of the film. This movie was never going to be great, but if we saw more of Mexico and less of giant heads this film might have been watchable.\")\n",
            "(0, 'Have I seen a worse movie? Perhaps only \"Manos: The Hands of Fate\" dragged more than \"Dukes\". I had more fun poking at the gigantic plot holes than the movie gave me at any point. Let\\'s touch on a few...<br /><br />There was a noticeable script death and rebirth when Sheev is talking to the Dukes and they don\\'t respond. He shrugs and moves on, since neither the Knoxville or Scott know what he\\'s talking about (nor do we). It was like the engine died and was restarted.<br /><br />The few times the General Lee flew through the air weren\\'t even that exciting. Nothing I haven\\'t seen on the TV Series.<br /><br />Very little chemistry between Knoxville and Scott. The best part was when Bo is upset at Luke for stealing the girl he liked. The only reason this works is that the script actually forshadowed it (although roughly). The rest of the time it seems distant and forced.<br /><br />Seann William Scott\\'s awful, horrendous accent (or lack thereof).<br /><br />I hated Willie Nelson\\'s performance. Were bad jokes supposed to be endearing? I wanted him to disappear.<br /><br />Jessica Simpson comes across splendidly on the big screen. She actually felt like one of the better actors in the film. That\\'s telling you how horrible this movie is. She\\'s a goddess.<br /><br />During the climax of the film, I was rooting for Boss Hogg and the bad guys to flatten all of Hazzard County, starting with Willie Nelson and his accomplices. A nuclear bomb would have sufficed.<br /><br />This is not meant to be a coherent dismantling of the film, but a release of frustration at the abysmal writing and execution of what could have been a truly heartwarming film.<br /><br />If only we could erase and start over...')\n",
            "(0, 'In 1914, Charlie Chaplin began making pictures. These were made for Mack Sennett (also known as \"Keystone Studios\") and were literally churned out in very rapid succession. The short comedies had very little structure and were completely ad libbed. As a result, the films, though popular in their day, were just awful by today\\'s standards. Many of them bear a strong similarity to home movies featuring obnoxious relatives mugging for the camera. Many others show the characters wander in front of the camera and do pretty much nothing. And, regardless of the outcome, Keystone sent them straight to theaters. My assumption is that all movies at this time must have been pretty bad, as the Keystone films with Chaplin were very successful.<br /><br />The Charlie Chaplin we know and love today only began to evolve later in Chaplin\\'s career with Keystone. By 1915, he signed a new lucrative contract with Essenay Studios and the films improved dramatically with Chaplin as director. However, at times these films were still very rough and not especially memorable. No, Chaplin as the cute Little Tramp was still evolving. In 1916, when he switched to Mutual Studios, his films once again improved and he became the more recognizable nice guy--in many of the previous films he was just a jerk (either getting drunk a lot, beating up women, provoking fights with innocent people, etc.). The final evolution of his Little Tramp to classic status occurred in the 1920s as a result of his full-length films.<br /><br />It\\'s interesting that this film is called TWENTY MINUTES OF LOVE since the film only lasts about 10 minutes! Oh well. The plot, what little there is, involves the Little Tramp in the park. A couple wants to neck but inexplicably, Charlie insists on practically sitting on the couple\\'s lap and really annoying them. I can\\'t understand why and the short consists of Charlie wandering about the park annoying these people and some others later in the film. Perhaps he was looking for a threesome, I don\\'t know. But the film lacks coherence and just isn\\'t particularly funny--even when people start slapping each other and pushing each other in the lake. A typical poor effort before Chaplin began to give his character a plot and personality.')\n",
            "(0, 'I remember watching \"Lost Missile\" (actually throwing a fit until my brother and several cousins at whose home I was an overnight guest agreed to watch it with me - I was, from time to time, the Eric Cartman of the 1960s - sorry, guys) and being somewhat embarrassed when the sustained wave of million-degree heat emerged as a plot device - even as a second-grader I knew that a mere missile just couldn\\'t carry the energy around for that much heat or devastation over more than the duration and limited radius of a nuclear detonation. <br /><br />My inflicting that turkey on loving relatives was a self-punishing crime.<br /><br />The film\\'s production values were very good. The acting isn\\'t bad (apart from the Shatnerism of the actor who played governor\\'s aide that someone else here mentioned).<br /><br />But the idea of a missile Easy-Baking the surface of the Earth by means of the heat of its exhaust... no.<br /><br />How\\'d the people at \"Mystery Science Theater 3000\" miss \"The Lost Missile,\" anyway? <br /><br />It\\'s a great classic of unintentional comedy - watch it if you want something to drink beer to some weekend.')\n",
            "(0, \"This insipid mini operetta featuring a Eddy-McDonald prototype in a Valentino scenario is so bad it becomes an endurance exercise after five minutes. It's silly from the get go as this brevity opens two military men discussing the lack of manliness in the son of one of the officers. In under a minute he is packed off to Morrocco where he lives a double life as the Red Shadow; the leader of an Arab tribe that would rather sing than fight.<br /><br />Alexander Gray and Bernice Clare possess fine light opera voices (with little acting ability) and there's a decent bass in there as well but the acting is so haphazard scenes so ill prepared you get the feeling they are making things up as they go along.<br /><br />This two reeler was part of a larger stage production that lists six writers. With more room to spoof and warble the show may have had some entertainment values but this rushed quickie is little more than an insult to an audience waiting for the feature presentation.\")\n",
            "(0, \"First lesson that some film makers (particularly those inspired by Hollywood) need to know - just 'style' does not sell. I guess Tashan when translated will mean style. Second, if you are hell bent on selling style, that does not spare you from having a decent story.<br /><br />Tashan has some story which could have sufficed with some better director. But it is not slick. For example, all three - Saif, Kareena and Akshay - are narrators at different points in the story. But this setup is not utilized to properly. There could have been a better mix and match of their narrations. Actions sequences are from the seventies.<br /><br />Cheoreography of the film is awful. I think Vaibhavi Merchant just sleep walked through this film. Vishal-Shekhar have put up a good score but it does not belong to this film. Why is there a sufi song (Dil Haara) in Tashan? Why is the cool Hinglish song (Dil Dance Maare) not on Anil Kapoor when he is the one who is English crazy? <br /><br />Akshay Kumar is the saving grace of the film. But he is in his stereotyped self. You won't mind missing this film.\")\n",
            "(0, \"Happy Go Lovely is a waste of everybody's time and talent including the audience. The lightness of the old-hat mistaken identity and faux scandal plot lines is eminently forgivable. Very few people watched these movies for their plots. But, they usually had some interesting minor characters involved in subplots -- not here. They usually had interesting choreography and breathtaking dancing and catchy songs. Not Happy Go Lovely. And Vera-Ellen as the female lead played the whole movie as a second banana looking desperately for a star to play off it -- and instead she was called upon to carry the movie, and couldn't do it. The Scottish locale was wasted. Usually automatically ubiquitous droll Scottish whimsy is absent. The photography was pedestrian. The musical numbers were pedestrian. Cesar Romero gives his usual professional performance, chewing up the scenery since no one else was doing his part, in the type of producer role essayed frequently by Walter Abel and Adolph Menjou. David Niven is just fine, and no one could do David Niven like David Niven. At the end of the day, if you adore Niven as I do, it's reason enough to waste 90 minutes on Happy Go Lovely. If not, skip it.\")\n",
            "(1, 'This is one of those Film\\'s/pilot that if you knew BattleStar Galactica it helps, but isn\\'t necessary. What makes this even more believable of a story than BSG is that this isn\\'t something so far away in the future. This has such a depth to it that it is quite astonishing it was not released theatrically. The leads could not have been chosen better in such experienced & quite talented actors. Eric Stoltz is superb as the father who will do anything to be re-united w/his daughter however real or not she is & he\\'ll do it no matter the cost. Paula Malcomson of \"Deadwood\" fame is terrific as his wife as well. You are not sure completely of his motives whether it\\'s love or money or both, but that is what makes this pilot even more intriguing. I see a star in the making of Zoe played by the relative unknown Alessandra Torresani & her performance. Esai Morales is terrific in his desire to see his loved one again & just how wrong to be even considering what he wants more than his moral objections. I didn\\'t think this would be a good idea when it was announced but from the pilot alone I am thrilled to see how we got to the BSG stage story. It\\'s great to see Adama as a child already being affected & influenced by the different sorts of Robots starting to permeate life at this stage. I just hope that they can keep up the stories so we can figure out even more how they got the the Humanoid typed robots. This is an almost perfect pilot & I hope they can keep up the fantastic storytelling. Even the Visual effects are better than most of the garbage you see on the big screen. If you haven\\'t gotten into BSG, @ least try this & I\\'ll bet you become a fan & will want to see how the BSG story came to be.')\n",
            "(1, 'If you are uninitiated to the Gundam world, this is a good place to start. If you are burned out on Star Wars or Star Trek, here is a compelling, realistic sci-fi series you can become immersed in. Not the simplistic boy-saves-world-in-giant robot story you might have expected, but rather a complex, emotionally compelling space war drama where the line between the \"good\" and \"bad\" guys is decidedly less than distinct.<br /><br />Gundam 0080 focuses on the story of Al Izuruha, a young, naive boy living in a neutral space colony. He spends his days daydreaming about Mobile Suits and playing war with his friends. During the course of this series, Al befriends an \"enemy\" soldier, Bernie Wiseman. By the end, little Al learns some hard lessons about the reality of war and the requisite suffering and sacrifice.<br /><br />I loved this OAV series, with its cool mecha designs, involving story, and likeable characters. I recommend this series to anyone who likes realistic SF anime, or to those who think anime is just silly or sexy entertainment.')\n",
            "(1, 'We toss around the term \"superstar\" way too lightly these days, but here\\'s one guy that truly deserves it.<br /><br />I was glued to the set this entire show. The song selection was perfect -- it only contained the songs I actually wanted to hear and cut in with documentary footage during the weaker new songs. I loved that the band was just a five guys on stage in a very minimalist environment. (With songs of this strength, you don\\'t need a circus to be entertained).<br /><br />The shots of the crowd were amazing, too. How many performers can affect the original Beatles fans (now in their 50\\'s and beyond), get young kids to jump up at the opening lines of \"Can\\'t Buy Me Love\" and impact everyone in between? <br /><br />While watching, I also realized that in the wake of John Lennon\\'s tragic death, Paul McCartney instantly became an afterthought. Paul not only lost John and George (no matter what their final relations were, it must be hard to lose someone with whom you changed the world), but he also lost his wife Linda and never really seemed to garner the acknowledgment Lennon\\'s murder received. I agree that Lennon\\'s murder was horrible, but only now did I realize that Paul was sort of forgotten in the aftermath. I was very happy that he\\'s found love again in Heather.<br /><br />As for those complaining about the audio/video quality, I had no complaints whatsoever; both were crystal clear on my set. I think these same people will complain about the quality of DVD when the next format comes out; they\\'ll never be satisfied.<br /><br />My only regret was not buying a ticket to this show when I had the chance. Thanks to this video I was able to enjoy it.<br /><br />When people remember John Lennon, they will first remember his murder and then his music. I now have a new appreciation for Paul McCartney, because, if nothing else, he will be remembered for his music first. And let\\'s hope another lunatic won\\'t change this, because the McCartney catalog is pretty good.')\n",
            "(1, 'Tom the cat, Jerry the mouse, and Spike the Dog (here called Butch, his third name, his second being \\'Killer\\') decide to sign a peace treaty to all love each other. It\\'s weird and a bit unnatural seeing them all buddy buddy like this and their friend\\'s seem to think so too. But by the end thanks to a disagreement over a steak, everything is back to normal and all is how it should be. This short is the second one of three on the new Spotlight DVD to be edited and I have no clue why this one was. This cartoon can be found on disc one of the Spotlight collection DVD of \"Tom & Jerry\" <br /><br />My Grade: B')\n",
            "(1, \"Red Eye is a thrilling film by the creator of Freddy Kreuger, Wes Craven. Wes Craven depicts the story of a regular hotel worker Lisa. After attending the funeral of her grandmother, she decides to take the red eye flight. During waiting, she meets this man named Jack Rippner, (how fffrreeaakkyy is that?) and they sort of become friends. Ironically, both sit right next to each other on this plane. Then this is when the horror starts. This movie is thrilling and to the weak hearted people who don't like thrilling/horror films, well lets say that its possible that they might pee in their pants. This is an excellent example of a bone shaking production. Wes Craven did well with this film. He chose the right actors, like Rachel McAdams, an intelligent, sexy girl who knows what she's doing and is cautious of everything when she's acting in a film. Cillian Murphy, the scary and horrifying actor who can chill your bones at his amazing acting being the bad character in this film, and his face can really widen your eyes. Wes Craven did an excellent job and I hope that he makes more films like this one.\")\n",
            "(1, 'When I found this film in my local videostore I expected it to be another cheesy American vampire film in the same vein of \"The Lost Boys\"(1987).To my surprise \"To Die for\" is a really good movie.It\\'s a little bit corny at times,but still there are enough stylish set-pieces and surprises to satisfy vampire enthusiasts.This is a perfect mix of romance and horror and it\\'s surprisingly gory at times.Highly recommended.')\n",
            "(1, 'This movie is great entertainment to watch with the wife or girlfriend. There are laughs galore and some very interesting little nudist stories going on here. The actresses are all very interesting and definitely worth watching in their natural beauty. Maslin beach life is full of diverse nudists and personality types. The Australian coast scenery is, simply, splendid to see. What a place to visit, to say the least, and one day it may become my hideaway. I really enjoy this movie and every time I watch it I enjoy it more. I would love to see more of these characters and I always wonder what became of them. Although the plot is somewhat soft, this movie is, of course, a great excuse to just sit back on the couch and enjoy the wonderful and famous Maslin beach with these wonderful nudists and their own personal stories.')\n",
            "(1, \"The third and last part of the Bourne trilogy (duh), is lacking a bit in the story department, but covers it with extensive action scenes! Twi in particular take up quite some of the running time and make this movie better.<br /><br />The director and star (Damon) themselves agreed that it was difficult to find a story for the last part, because the end of the second movie was quite ... advanced story-wise. How they got around that? The action scenes, for once, but they did another thing too, which I can't reveal, because that would be a spoiler. But if you watch the movie, than you'll notice it! Funnily enough I read, that this adaptation of the Bourne books is the least accurate of all three films .. if that means anything to you :o)\")\n",
            "(1, \"This has to be one of my favourite movies of all time. The dialogue, with the constant use of puns is very tight, the cast are superb, and the plot is highly original.<br /><br />Don't take my word for it - watch this movie and enjoy it for yourself.\")\n",
            "(1, 'Life Begins - and ends - in a typical 1930\\'s maternity / recovery ward, where we view 48 hours in the lives of several high risk pregnant women, played by Loretta Young, Glenda Farrell, Clara Blandick (Aunty Em???), Vivienne Osborne, Dorothy Tree, and Gloria Shea, as they await to give birth. While the film features plot devices which seem far fetched today when maternity wards are much more controlled and restricted, it does offer us a look back in time to see what giving birth in a typical city hospital in 1932 was like for our grandmothers and great-grandmothers. I found the film fascinating and exceptionally moving.<br /><br />Oddly enough, the most outstanding performance in this film comes from a male cast member, young Eric Linden as Jed Sutton, Grace\\'s (Loretta Young) husband. What an actor! As a first time father, Jed is distraught and uneasy with hospital staff who seem to brush off his concerns about his wife as they might brush crumbs off a cafeteria table. I felt his every concern keenly. I\\'d like to see more of this actor\\'s work. He had a very emotional voice, which was used to unforgettable effect in Gone With The Wind. In that film Eric played the young soldier whose leg was amputated without anesthesia, who screamed \"Don\\'t cut! Don\\'t cut!\" as Scarlett fled the hospital in horror. Chilling! Another great performance is from Aline MacMahon, who plays Miss Bowers, the nurse. Her character is a salt of the earth type, the kind of nurse we all hope to get for our hospital stays, who breaks the hospital rules constantly in order to show a more humane side of the medical profession.<br /><br />Loretta Young did another superb acting job here as well, a very authentic and deeply felt performance as Grace. My, she is great in these precodes, I\\'ve really grown to appreciate her more as an actress the last few months.<br /><br />Glenda Farrell played her role of a shrill unwed mother a little over the top for my taste (didn\\'t anyone know back in 1932 that swigging brandy from a hot water bottle might be hazardous to unborn babies\\' health?) but her character redeems herself in the end.<br /><br />Also in the cast was an uncredited Gilbert Roland, silent movie star, as a grieving Italian husband. His screen time was brief, but notable.<br /><br />Life Begins is a must-see precode, try to catch it sometime on TCM, but remember to bring a few hankies to cry into. 9 out of 10.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_label = {0: \" negative review\", 1: \"positive review\"}\n"
      ],
      "metadata": {
        "id": "kIVNXUJLLjHq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "num_class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnHWuxTTLliU",
        "outputId": "fb092d73-42e0-4b3e-b4e2-84d20a47a8bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "def yield_tokens(data_iter):\n",
        "    \"\"\"Yield tokens for each data sample.\"\"\"\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)"
      ],
      "metadata": {
        "id": "MvXMQ3ayLtWm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The following code loads a pretrained word embedding model called GloVe into a variable called glove_embedding."
      ],
      "metadata": {
        "id": "TJ3SpTKNL8gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Note that GloVe embeddings are typically downloaded using:\n",
        "#glove_embedding = GloVe(name=\"6B\", dim=100)\n",
        "# However, the GloVe server is frequently down. The code below offers a workaround\n",
        "\n",
        "\n",
        "class GloVe_override(Vectors):\n",
        "    url = {\n",
        "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
        "        url = self.url[name]\n",
        "        name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
        "        #name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
        "        super(GloVe_override, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "class GloVe_override2(Vectors):\n",
        "    url = {\n",
        "        \"6B\": \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/tQdezXocAJMBMPfUJx_iUg/glove-6B.zip\",\n",
        "    }\n",
        "\n",
        "    def __init__(self, name=\"6B\", dim=100, **kwargs) -> None:\n",
        "        url = self.url[name]\n",
        "        #name = \"glove.{}.{}d.txt\".format(name, str(dim))\n",
        "        name = \"glove.{}/glove.{}.{}d.txt\".format(name, name, str(dim))\n",
        "        super(GloVe_override2, self).__init__(name, url=url, **kwargs)\n",
        "\n",
        "try:\n",
        "    glove_embedding = GloVe_override(name=\"6B\", dim=100)\n",
        "except:\n",
        "    try:\n",
        "        glove_embedding = GloVe_override2(name=\"6B\", dim=100)\n",
        "    except:\n",
        "        glove_embedding = GloVe(name=\"6B\", dim=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6urkTg3L9hm",
        "outputId": "752f85f6-1bc5-4733-b335-28d064dfd13d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove-6B.zip: 134MB [00:13, 9.98MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:25<00:00, 15930.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe,vocab\n",
        "# Build vocab from glove_vectors\n",
        "vocab = vocab(glove_embedding .stoi, 0,specials=('<unk>', '<pad>'))\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "metadata": {
        "id": "UzkC_j6wPEkS"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=len(vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTG8Ovc_O8gH",
        "outputId": "7b2802e8-0d65-4171-c853-b101d4d55c41"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400002"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab(['he'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCn-lBAvPKA7",
        "outputId": "aa7e69a8-0f36-470c-b483-4a4a1166c3ca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[20]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset splits"
      ],
      "metadata": {
        "id": "13ElI6syPMf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the training and testing iterators to map-style datasets.\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# Determine the number of samples to be used for training and validation (5% for validation).\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "\n",
        "# Randomly split the training dataset into training and validation datasets using `random_split`.\n",
        "# The training dataset will contain 95% of the samples, and the validation dataset will contain the remaining 5%.\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])"
      ],
      "metadata": {
        "id": "3ZuQgujIPNH6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_train = int(len(train_dataset) * 0.05)\n",
        "split_train_, _ = random_split(split_train_, [num_train, len(split_train_) - num_train])"
      ],
      "metadata": {
        "id": "ZlaTNKb8PY-S"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deN3qjGpPa0R",
        "outputId": "906e333c-4985-4222-a717-6a49ac7c31b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data loader"
      ],
      "metadata": {
        "id": "r9eRIDJ_PmFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_pipeline(x):\n",
        "    return vocab(tokenizer(x))"
      ],
      "metadata": {
        "id": "iupiTAyNPm5V"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for _label, _text in batch:\n",
        "\n",
        "        label_list.append(_label)\n",
        "        text_list.append(torch.tensor(text_pipeline(_text), dtype=torch.int64))\n",
        "\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "\n",
        "    return label_list.to(device), text_list.to(device)"
      ],
      "metadata": {
        "id": "2C97e9tkTADX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "valid_dataloader = DataLoader(\n",
        "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")"
      ],
      "metadata": {
        "id": "G4m7evi_TDG3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label,seqence=next(iter(valid_dataloader))\n",
        "label,seqence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4XPfiWwTGKL",
        "outputId": "ef9f6ffd-380c-4f68-d2c5-06df455d1cfd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
              "         1, 1, 1, 0, 0, 0, 1, 1]),\n",
              " tensor([[    2,   359,   630,  ...,     0,     0,     0],\n",
              "         [   43,   824,    39,  ...,     0,     0,     0],\n",
              "         [   65,    16,    86,  ...,     0,     0,     0],\n",
              "         ...,\n",
              "         [  104,    16,   220,  ...,     0,     0,     0],\n",
              "         [ 6036, 46884,  1058,  ...,     0,     0,     0],\n",
              "         [   43,   648,    22,  ...,     0,     0,     0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a class called Net that represents a text classifier based on a PyTorch TransformerEncoder.\n",
        "The constructor takes the following arguments:\n",
        "\n",
        "- `num_class`: The number of classes to classify\n",
        "- `vocab_size`: The size of the vocabulary\n",
        "- `freeze`: Whether to freeze the embedding layer\n",
        "- `nhead`: The number of heads in the transformer encoder\n",
        "- `dim_feedforward`: The dimension of the feedforward layer in the transformer encoder\n",
        "- `num_layers`: The number of transformer encoder layers\n",
        "- `dropout`: The dropout rate\n",
        "- `activation`: The activation function to use in the transformer encoder\n",
        "- `classifier_dropout`: The dropout rate for the classifier\n",
        "\n",
        "**Attributes:**\n",
        "\n",
        "- `emb`: An embedding layer that maps each word in the vocabulary to a dense vector representation\n",
        "- `pos_encoder`: A positional encoding layer that adds positional information to the word vectors\n",
        "- `transformer_encoder`: A transformer encoder layer that processes the sequence of word vectors and extracts high-level features\n",
        "- `classifier`: A linear layer that maps the output of the transformer encoder to the desired number of classes\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gAxBYsTFTOpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    \"\"\"\n",
        "    Text classifier based on a pytorch TransformerEncoder.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "\n",
        "        self,\n",
        "        num_class,vocab_size,\n",
        "        freeze=True,\n",
        "        nhead=2,\n",
        "        dim_feedforward=128,\n",
        "        num_layers=2,\n",
        "        dropout=0.1,\n",
        "        activation=\"relu\",\n",
        "        classifier_dropout=0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        #self.emb = embedding=nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n",
        "        self.emb = nn.Embedding.from_pretrained(glove_embedding.vectors,freeze=freeze)\n",
        "        embedding_dim = self.emb.embedding_dim\n",
        "\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=embedding_dim,\n",
        "            dropout=dropout,\n",
        "            vocab_size=vocab_size,\n",
        "        )\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embedding_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers,\n",
        "        )\n",
        "        self.classifier = nn.Linear(embedding_dim, num_class)\n",
        "        self.d_model = embedding_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "nslPrKM-TQD-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Net(num_class=2,vocab_size=vocab_size).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6iadMtdUQbg",
        "outputId": "2fb851e1-6a13-478a-c2b4-4390fd19998a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (emb): Embedding(400000, 100)\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-1): 2 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=100, out_features=128, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=128, out_features=100, bias=True)\n",
              "        (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): Linear(in_features=100, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text, text_pipeline, model):\n",
        "    with torch.no_grad():\n",
        "        text = torch.unsqueeze(torch.tensor(text_pipeline(text)),0).to(device)\n",
        "        model.to(device)\n",
        "        output = model(text)\n",
        "        return imdb_label[output.argmax(1).item()]"
      ],
      "metadata": {
        "id": "BDU5KHGMUVt-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict(\"I like sports and stuff\", text_pipeline, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "cs2PyZzvUYAb",
        "outputId": "755de013-8a69-4d46-e9fe-2a5028f68e89"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'positive review'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(dataloader, model_eval):\n",
        "    model_eval.eval()\n",
        "    total_acc, total_count= 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for label, text in tqdm(dataloader):\n",
        "            label, text = label.to(device), text.to(device)\n",
        "            output = model_eval(text)\n",
        "            predicted = torch.max(output.data, 1)[1]\n",
        "            total_acc += (predicted == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc / total_count"
      ],
      "metadata": {
        "id": "WppVptLNUcaY"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_no_tqdm(dataloader, model_eval):\n",
        "    model_eval.eval()\n",
        "    total_acc, total_count= 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for label, text in dataloader:\n",
        "            label, text = label.to(device), text.to(device)\n",
        "            output = model_eval(text)\n",
        "            predicted = torch.max(output.data, 1)[1]\n",
        "            total_acc += (predicted == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc / total_count"
      ],
      "metadata": {
        "id": "gZVgCRFVUf0W"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(test_dataloader, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7N249-QOUiSf",
        "outputId": "9449e716-9035-4a3f-c377-644684640393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 723/782 [04:36<00:19,  3.04it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "CwCPYZmQUldK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, optimizer, criterion, train_dataloader, valid_dataloader,  epochs=1000, save_dir=\"\", file_name=None):\n",
        "    cum_loss_list = []\n",
        "    acc_epoch = []\n",
        "    acc_old = 0\n",
        "    model_path = os.path.join(save_dir, file_name)\n",
        "    acc_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + \"_acc\")\n",
        "    loss_dir = os.path.join(save_dir, os.path.splitext(file_name)[0] + \"_loss\")\n",
        "    time_start = time.time()\n",
        "\n",
        "    for epoch in tqdm(range(1, epochs + 1)):\n",
        "        model.train()\n",
        "        #print(model)\n",
        "        #for parm in model.parameters():\n",
        "        #    print(parm.requires_grad)\n",
        "\n",
        "        cum_loss = 0\n",
        "        for idx, (label, text) in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            label, text = label.to(device), text.to(device)\n",
        "\n",
        "            predicted_label = model(text)\n",
        "            loss = criterion(predicted_label, label)\n",
        "            loss.backward()\n",
        "            #print(loss)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "            optimizer.step()\n",
        "            cum_loss += loss.item()\n",
        "        print(f\"Epoch {epoch}/{epochs} - Loss: {cum_loss}\")\n",
        "\n",
        "        cum_loss_list.append(cum_loss)\n",
        "        accu_val = evaluate_no_tqdm(valid_dataloader,model)\n",
        "        acc_epoch.append(accu_val)\n",
        "\n",
        "        if model_path and accu_val > acc_old:\n",
        "            print(accu_val)\n",
        "            acc_old = accu_val\n",
        "            if save_dir is not None:\n",
        "                pass\n",
        "                #print(\"save model epoch\",epoch)\n",
        "                #torch.save(model.state_dict(), model_path)\n",
        "                #save_list_to_file(lst=acc_epoch, filename=acc_dir)\n",
        "                #save_list_to_file(lst=cum_loss_list, filename=loss_dir)\n",
        "\n",
        "    time_end = time.time()\n",
        "    print(f\"Training time: {time_end - time_start}\")"
      ],
      "metadata": {
        "id": "7XDSdGXRUmhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Train IMDB"
      ],
      "metadata": {
        "id": "_c-8Zh0JUzln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LR=1\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "save_dir = \"\"\n",
        "file_name = \"model_IMDB dataset small2.pth\"\n",
        "train_model(model=model,\n",
        "            optimizer=optimizer,\n",
        "            criterion=criterion,\n",
        "            train_dataloader=train_dataloader,\n",
        "            valid_dataloader=valid_dataloader,\n",
        "            epochs=2,\n",
        "            save_dir=save_dir,\n",
        "            file_name=file_name\n",
        "           )"
      ],
      "metadata": {
        "id": "MVIiYWFQUyvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Fine-tune the final layer only"
      ],
      "metadata": {
        "id": "Z0OxE7LHVDC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9c3Dh2O_jsYBShBuchUNlg/model-AG%20News%20small1.pth')\n",
        "model_fine2 = Net(vocab_size=vocab_size, num_class=4).to(device)\n",
        "model_fine2.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))"
      ],
      "metadata": {
        "id": "zcY5U2OIVDs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all layers in the model\n",
        "for param in model_fine2.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "brEOIR6WVG2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim=model_fine2.classifier.in_features"
      ],
      "metadata": {
        "id": "vhRWOxY5VImH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fine2.classifier = nn.Linear(dim, 2)"
      ],
      "metadata": {
        "id": "uU6wHRLRVKEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_fine2.to(device)\n"
      ],
      "metadata": {
        "id": "dXJJWBI9VLbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LR=1\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model_fine2.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "save_dir = \"\"\n",
        "file_name = \"model_fine2.pth\"\n",
        "train_model(model=model_fine2, optimizer=optimizer, criterion=criterion, train_dataloader=train_dataloader, valid_dataloader=valid_dataloader,  epochs=2,  save_dir=save_dir ,file_name=file_name )\n"
      ],
      "metadata": {
        "id": "yTbhYMGRVOWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "acc_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/UdR3ApQnxSeV2mrA0CbiLg/model-fine2-acc')\n",
        "loss_urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/rWGDIF-uL2dEngWcIo9teQ/model-fine2-loss')\n",
        "acc_epoch = pickle.load(acc_urlopened)\n",
        "cum_loss_list = pickle.load(loss_urlopened)\n",
        "plot(cum_loss_list,acc_epoch)"
      ],
      "metadata": {
        "id": "5BAZy-RqVP5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlopened = urlopen('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/B-1H6lpDg-A0zRwpB6Ek2g/model-fine2.pth')\n",
        "model_fine2_ = Net(vocab_size=vocab_size, num_class=2).to(device)\n",
        "model_fine2_.load_state_dict(torch.load(io.BytesIO(urlopened.read()), map_location=device))\n",
        "evaluate(test_dataloader, model_fine2_)"
      ],
      "metadata": {
        "id": "Mt3-_0nyVTJe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}